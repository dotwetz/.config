#compdef llama-cli

local arguments

arguments=(
  '(-h --help --usage)'{-h,--help,--usage}'[print usage and exit]'
  '--version[show version and build info]'
  '(-v --verbose)'{-v,--verbose}'[print verbose information]'
  '--verbosity[set specific verbosity level]:verbosity level'
  '(-t --threads)'{-t,--threads}'[number of threads to use]:threads'
  '(-tb --threads-batch)'{-tb,--threads-batch}'[number of threads to use for batch]:batch threads'
  '(-C --cpu-mask)'{-C,--cpu-mask}'[CPU affinity mask]:cpu mask'
  '(-Cr --cpu-range)'{-Cr,--cpu-range}'[range of CPUs for affinity]:cpu range'
  '--cpu-strict[use strict CPU placement]:strict placement(0|1)'
  '--prio[set process/thread priority]:priority level(0-normal|1-medium|2-high|3-realtime)'
  '--poll[use polling level to wait for work]:poll level(0..100)'
  '(-Cb --cpu-mask-batch)'{-Cb,--cpu-mask-batch}'[CPU affinity mask for batch]:cpu mask'
  '(-Crb --cpu-range-batch)'{-Crb,--cpu-range-batch}'[range of CPUs for batch]:batch cpu range'
  '--cpu-strict-batch[use strict CPU placement for batch]:strict placement(0|1)'
  '--prio-batch[set process/thread priority for batch]:priority level(0-normal|1-medium|2-high|3-realtime)'
  '--poll-batch[use polling to wait for work in batch]:poll batch(0|1)'
  '(-c --ctx-size)'{-c,--ctx-size}'[size of the prompt context]:context size'
  '(-n --predict --n-predict)'{-n,--predict,--n-predict}'[number of tokens to predict]:tokens'
  '(-b --batch-size)'{-b,--batch-size}'[logical maximum batch size]:batch size'
  '(-ub --ubatch-size)'{-ub,--ubatch-size}'[physical maximum batch size]:ubatch size'
  '--keep[number of tokens to keep]:tokens'
  '(-fa --flash-attn)'{-fa,--flash-attn}'[enable Flash Attention]'
  '(-p --prompt)'{-p,--prompt}'[prompt to start generation]:prompt'
  '--no-perf[disable internal performance timings]'
  '(-f --file)'{-f,--file}'[file containing the prompt]:file:_files'
  '(-bf --binary-file)'{-bf,--binary-file}'[binary file containing the prompt]:file:_files'
  '(-e --escape)'{-e,--escape}'[process escapes sequences]'
  '--no-escape[do not process escape sequences]'
  '--rope-scaling[RoPE frequency scaling method]:scaling method(none|linear|yarn)'
  '--rope-scale[RoPE context scaling factor]:scaling factor'
  '--rope-freq-base[RoPE base frequency]:frequency'
  '--yarn-orig-ctx[YaRN original context size]:context size'
  '--yarn-ext-factor[YaRN extrapolation mix factor]:factor'
  '--yarn-attn-factor[YaRN attention factor]:factor'
  '--yarn-beta-slow[YaRN high correction dim or alpha]:beta slow'
  '--yarn-beta-fast[YaRN low correction dim or beta]:beta fast'
  '--grp-attn-n[group-attention factor]:attention factor'
  '--grp-attn-w[group-attention width]:attention width'
  '--dump-kv-cache[verbose print of the KV cache]'
  '--no-kv-offload[disable KV offload]'
  '--cache-type-k[KV cache data type for K]:data type'
  '--cache-type-v[KV cache data type for V]:data type'
  '--defrag-thold[KV cache defragmentation threshold]:threshold'
  '--parallel[number of parallel sequences to decode]:sequences'
  '--mlock[keep model in RAM]'
  '--no-mmap[do not memory-map model]'
  '--numa[NUMA optimizations]:optimization(distribute|isolate|numactl)'
  '--gpu-layers[store layers in VRAM]:layers'
  '--split-mode[model split across multiple GPUs]:split mode(none|layer|row)'
  '--tensor-split[fraction of model to offload to GPUs]:fractions'
  '--main-gpu[main GPU for the model]:GPU index'
  '--check-tensors[check model tensor data]'
  '--override-kv[override model metadata by key]:key=value'
  '--lora[path to LoRA adapter]:file:_files'
  '--lora-scaled[path to LoRA adapter with scaling]:file:_files scale'
  '--control-vector[path to control vector]:file:_files'
  '--model[path to model]:file:_files'
  '--model-url[model download URL]:url'
  '--hf-repo[Hugging Face model repository]:repository'
  '--hf-file[Hugging Face model file]:file:_files'
  '--hf-token[Hugging Face access token]:token'
  '--logdir[path to save YAML logs]:logdir:_files'
  '--samplers[samplers used for generation]:samplers'
  '(-s --seed)'{-s,--seed}'[RNG seed]:seed'
  '--sampling-seq[simplified sampler sequence]:sequence'
  '--ignore-eos[ignore end of stream token]'
  '--penalize-nl[penalize newline tokens]'
  '--temp[temperature]:temperature'
  '--top-k[top-k sampling]:k value'
  '--top-p[top-p sampling]:p value'
  '--min-p[min-p sampling]:p value'
  '--tfs[tail free sampling parameter z]:z value'
  '--typical[locally typical sampling parameter p]:p value'
  '--repeat-last-n[last n tokens to consider]:tokens'
  '--repeat-penalty[penalize repeat sequence of tokens]:penalty'
  '--presence-penalty[presence penalty]:penalty'
  '--frequency-penalty[frequency penalty]:penalty'
  '--mirostat[use Mirostat sampling]:Mirostat version'
  '--mirostat-lr[Mirostat learning rate]:learning rate'
  '--mirostat-ent[Mirostat target entropy]:entropy'
  '--logit-bias[modify token likelihood]:token id(+/-)bias'
  '--grammar[BNF-like grammar to constrain generations]:grammar'
  '--grammar-file[file to read grammar from]:file:_files'
  '--json-schema[JSON schema to constrain generations]:schema'
  '--verbose-prompt[print verbose prompt]'
  '--no-display-prompt[do not print prompt]'
  '--color[colorise output]'
  '--print-token-count[print token count every N tokens]:count'
  '--prompt-cache[file to cache prompt state]:file:_files'
  '--reverse-prompt[halt generation at prompt]:prompt'
  '--special[enable special tokens output]'
  '--conversation[run in conversation mode]'
  '--interactive[run in interactive mode]'
  '--multiline-input[allow multiline input]'
  '--no-warmup[skip warming up the model]'
  '--chat-template[set custom jinja chat template]:template'
  '--simple-io[use basic IO]'
)

_arguments $arguments